{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Adaptive Anomaly Detection\n",
    "\n",
    "This notebook addresses the issue of increasing anomaly scores over time by implementing:\n",
    "1. Sliding window approach\n",
    "2. Proper data normalization\n",
    "3. Alternative anomaly detection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "\n",
    "# Assuming you have these custom modules\n",
    "from dataset import get_df_action, get_features_ts, get_train_test_data\n",
    "from plots import plot_anomalies, plot_anomalies_over_time\n",
    "\n",
    "# Set style for matplotlib\n",
    "plt.style.use(\"Solarize_Light2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the root directory of the dataset\n",
    "ROOTDIR_DATASET_NORMAL =  '../../dataset/normal'#'/content/drive/MyDrive/Kuka_v1/normal'\n",
    "ROOTDIR_DATASET_ANOMALY = '../../dataset/collisions'#'/content/drive/MyDrive/Kuka_v1/collisions'\n",
    "\n",
    "# TF_ENABLE_ONEDNN_OPTS=0 means that the model will not use the oneDNN library for optimization\n",
    "\n",
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freq = '1.0'\n",
    "freq = '0.1'\n",
    "#freq = '0.01'\n",
    "#freq = '0.005'\n",
    "\n",
    "# NORMAL DATA\n",
    "filepath_csv = [os.path.join(ROOTDIR_DATASET_NORMAL, f\"rec{r}_20220811_rbtc_{freq}s.csv\") for r in [0, 2, 3, 4]]\n",
    "filepath_meta = [os.path.join(ROOTDIR_DATASET_NORMAL, f\"rec{r}_20220811_rbtc_{freq}s.metadata\") for r in [0, 2, 3, 4]]\n",
    "df_action, df, df_meta, action2int = get_df_action(filepath_csv, filepath_meta)\n",
    "\n",
    "\n",
    "# COLLISION DATA\n",
    "xls = pd.ExcelFile(os.path.join(ROOTDIR_DATASET_ANOMALY, \"20220811_collisions_timestamp.xlsx\"))\n",
    "collision_rec1 = pd.read_excel(xls, 'rec1')\n",
    "collision_rec5 = pd.read_excel(xls, 'rec5')\n",
    "\n",
    "collisions = pd.concat([collision_rec1, collision_rec5])\n",
    "collisions_init = collisions[collisions['Inizio/fine'] == \"i\"].Timestamp - pd.to_timedelta([2] * len(collisions[collisions['Inizio/fine'] == \"i\"].Timestamp), 'h')\n",
    "\n",
    "filepath_csv = [os.path.join(ROOTDIR_DATASET_ANOMALY, f\"rec{r}_collision_20220811_rbtc_{freq}s.csv\") for r in [1, 5]]\n",
    "filepath_meta = [os.path.join(ROOTDIR_DATASET_ANOMALY, f\"rec{r}_collision_20220811_rbtc_{freq}s.metadata\") for r in [1, 5]]\n",
    "df_action_collision, df_collision, df_meta_collision, action2int_collision = get_df_action(filepath_csv, filepath_meta)\n",
    "\n",
    "filepath_csv = [os.path.join(ROOTDIR_DATASET_ANOMALY, f\"rec1_collision_20220811_rbtc_{freq}s.csv\")]\n",
    "filepath_meta = [os.path.join(ROOTDIR_DATASET_ANOMALY, f\"rec1_collision_20220811_rbtc_{freq}s.metadata\")]\n",
    "df_action_collision_1, df_collision_1, df_meta_collision_1, action2int_collision_1 = get_df_action(filepath_csv, filepath_meta)\n",
    "\n",
    "filepath_csv = [os.path.join(ROOTDIR_DATASET_ANOMALY, f\"rec5_collision_20220811_rbtc_{freq}s.csv\")]\n",
    "filepath_meta = [os.path.join(ROOTDIR_DATASET_ANOMALY, f\"rec5_collision_20220811_rbtc_{freq}s.metadata\")]\n",
    "df_action_collision_5, df_collision_5, df_meta_collision_5, action2int_collision_5 = get_df_action(filepath_csv, filepath_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "frequency = 1/float(freq)\n",
    "df_features = get_features_ts(\"statistical\", df_action, df_meta, frequency, action2int)\n",
    "df_features_collision = get_features_ts(\"statistical\", df_action_collision, df_meta_collision, frequency, action2int_collision)\n",
    "df_features_collision_1 = get_features_ts(\"statistical\", df_action_collision_1, df_meta_collision_1, frequency, action2int_collision_1)\n",
    "df_features_collision_5 = get_features_ts(\"statistical\", df_action_collision_5, df_meta_collision_5, frequency, action2int_collision_5)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = get_train_test_data(df_features, df_features_collision, full_normal=True)\n",
    "X_train1, y_train1, X_test1, y_test1 = get_train_test_data(df_features, df_features_collision_1, full_normal=True)\n",
    "X_train5, y_train5, X_test5, y_test5 = get_train_test_data(df_features, df_features_collision_5, full_normal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data (as in your original notebook)\n",
    "# ...\n",
    "\n",
    "# After preprocessing, you should have X_train and X_test\n",
    "# For this example, let's assume X_train and X_test are DataFrames\n",
    "# with a DatetimeIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sliding Window Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_anomaly_detection(X_train, X_test, window_size=1000, stride=100):\n",
    "    anomaly_scores = []\n",
    "    scaler = StandardScaler()\n",
    "    model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "    \n",
    "    for i in range(0, len(X_test), stride):\n",
    "        # Select window of training data\n",
    "        train_window = X_train.iloc[max(0, i-window_size):i]\n",
    "        \n",
    "        # Normalize data\n",
    "        train_scaled = scaler.fit_transform(train_window)\n",
    "        test_scaled = scaler.transform(X_test.iloc[i:i+stride])\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(train_scaled, train_scaled)\n",
    "        \n",
    "        # Compute reconstruction error\n",
    "        test_pred = model.predict(test_scaled)\n",
    "        mse = np.mean(np.power(test_scaled - test_pred, 2), axis=1)\n",
    "        anomaly_scores.extend(mse)\n",
    "    \n",
    "    return np.array(anomaly_scores)\n",
    "\n",
    "sliding_window_scores = sliding_window_anomaly_detection(X_train, X_test)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(sliding_window_scores)\n",
    "plt.title('Anomaly Scores using Sliding Window Approach')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Anomaly Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Alternative Anomaly Detection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolation_forest_detection(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    clf = IsolationForest(contamination=0.1, random_state=42)\n",
    "    clf.fit(X_train_scaled)\n",
    "    \n",
    "    # Negative scores represent anomalies\n",
    "    return -clf.score_samples(X_test_scaled)\n",
    "\n",
    "def local_outlier_factor_detection(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    clf = LocalOutlierFactor(n_neighbors=20, contamination=0.1, novelty=True)\n",
    "    clf.fit(X_train_scaled)\n",
    "    \n",
    "    # Negative scores represent anomalies\n",
    "    return -clf.score_samples(X_test_scaled)\n",
    "\n",
    "if_scores = isolation_forest_detection(X_train, X_test)\n",
    "lof_scores = local_outlier_factor_detection(X_train, X_test)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(sliding_window_scores)\n",
    "plt.title('XGBoost with Sliding Window')\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(if_scores)\n",
    "plt.title('Isolation Forest')\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(lof_scores)\n",
    "plt.title('Local Outlier Factor')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_anomaly_detection(y_true, anomaly_scores):\n",
    "    auc_roc = roc_auc_score(y_true, anomaly_scores)\n",
    "    return auc_roc\n",
    "\n",
    "# Assuming y_test is your ground truth anomaly labels\n",
    "print(\"AUC-ROC Scores:\")\n",
    "print(f\"XGBoost with Sliding Window: {evaluate_anomaly_detection(y_test, sliding_window_scores):.4f}\")\n",
    "print(f\"Isolation Forest: {evaluate_anomaly_detection(y_test, if_scores):.4f}\")\n",
    "print(f\"Local Outlier Factor: {evaluate_anomaly_detection(y_test, lof_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyzing Time Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_over_time(X, feature_name):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(X.index, X[feature_name])\n",
    "    plt.title(f'{feature_name} over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.show()\n",
    "\n",
    "# Plot a few important features over time\n",
    "important_features = ['feature1', 'feature2', 'feature3']  # Replace with actual feature names\n",
    "for feature in important_features:\n",
    "    plot_feature_over_time(X_test, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion and Next Steps\n",
    "\n",
    "Based on the results of these different approaches, we can draw the following conclusions:\n",
    "\n",
    "1. If the sliding window approach significantly reduced the trend of increasing anomaly scores, it suggests that there might be drift in your data over time.\n",
    "\n",
    "2. If Isolation Forest or Local Outlier Factor perform better (higher AUC-ROC), it might indicate that these methods are more suitable for your specific dataset.\n",
    "\n",
    "3. The feature plots over time can help identify if certain features are causing the time-dependent behavior.\n",
    "\n",
    "Next steps:\n",
    "1. Fine-tune the best performing model (adjust window size, number of estimators, etc.)\n",
    "2. Investigate any features showing strong time dependency and consider how to handle them (e.g., detrending, differencing)\n",
    "3. Consider implementing a more advanced time series anomaly detection method if the time component is crucial\n",
    "4. Regularly retrain your model on recent data to adapt to potential concept drift"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
